{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Serverless ETL Pipeline using AWS Lambda, Glue Jobs, Athena\n",
        "\n",
        "@author: Glad Nayak <br>\n",
        "@email: gladn94@gmail.com"
      ],
      "metadata": {
        "id": "ExO3UZGqL7CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0: Prerequisites"
      ],
      "metadata": {
        "id": "jyYwD7XPQKC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y16RAXpDZUCs"
      },
      "outputs": [],
      "source": [
        "# install required packages in custom environment\n",
        "!pip install -q boto3 awscli aiohttp yarl log4p pyspark findspark\n",
        "\n",
        "# initialize spark\n",
        "from IPython.display import clear_output \n",
        "import os\n",
        "import findspark\n",
        "import sys\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht791CjgJXfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b158668f-eead-455f-88c4-20519aceb453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GH_BUCKET_NAME=github-activitiy-gb\n",
            "env: AWS_PROFILE=glad\n",
            "env: AWS_DEFAULT_REGION=ap-south-1\n"
          ]
        }
      ],
      "source": [
        "# setting environment variables\n",
        "%env GH_BUCKET_NAME=github-activitiy-gb\n",
        "%env AWS_PROFILE=glad\n",
        "%env AWS_DEFAULT_REGION=ap-south-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXBjlQ5LZjgC"
      },
      "outputs": [],
      "source": [
        "# configure cli access to AWS account \n",
        "!aws configure --profile glad\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 ls ${GH_BUCKET_NAME} --profile glad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYSgu9BQRRTk",
        "outputId": "015b27d1-5607-4ed8-abf9-8b1895477832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           PRE landing/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u49BL0SvaPMD"
      },
      "source": [
        "## 1: Extract Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9tPs-PoOhxiw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da4198f-904f-470f-b82e-7843eff764bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing extract_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile extract_data.py\n",
        "from aiohttp import ClientSession\n",
        "import asyncio\n",
        "import os\n",
        "import yarl\n",
        "import boto3\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta as td\n",
        "\n",
        "async def upload_file(session: ClientSession, s3_client: boto3.client, key: str) -> dict:\n",
        "    \"\"\"\n",
        "    Downloads file from Github archive and uploads it to AWS S3 bucket in async way\n",
        "    S3 bucket name is set in environment variable \n",
        "    @session aiohttp.ClientSession object\n",
        "    @s3_client client to connect with AWS S3\n",
        "    @key filename to download from Github archive\n",
        "    \"\"\"\n",
        "    BUCKET_NAME = os.environ.get('GH_BUCKET_NAME')\n",
        "\n",
        "    url = yarl.URL(f'https://data.gharchive.org/{key}.json.gz', encoded=True)\n",
        "\n",
        "    async with session.get(url, allow_redirects=False) as response:\n",
        "\n",
        "        try:\n",
        "            print('[extract] downloading data from', response.url)\n",
        "            stream_bytes = await response.read()\n",
        "\n",
        "            print('[extract] loading data to bucket', BUCKET_NAME)\n",
        "            \n",
        "            s3_client.put_object(\n",
        "                Bucket=BUCKET_NAME,\n",
        "                Key=f'landing/{key}.json.gz',\n",
        "                Body=stream_bytes,\n",
        "\n",
        "        )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "async def main():\n",
        "\n",
        "    START_DATE_RANGE = os.environ.setdefault('START_DATE_RANGE', '2021-01-01')\n",
        "    END_DATE_RANGE = os.environ.setdefault('END_DATE_RANGE', '2021-01-03')\n",
        "    boto3.setup_default_session(profile_name='glad')\n",
        "    \n",
        "    filenames = [\n",
        "                dt.strftime(dr + td(hours=hour),'%Y-%m-%d-%-H') \n",
        "                for hour in range(1, 25) \n",
        "                for dr in pd.date_range(START_DATE_RANGE, END_DATE_RANGE)\n",
        "            ]\n",
        "\n",
        "    s3_client = boto3.client('s3')\n",
        "    async with ClientSession() as session:\n",
        "        tasks = [upload_file(session, s3_client, filename) for filename in filenames]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        print(f'[extract] Uploaded {sum(result for result in results if type(result) == bool)} files successfully')\n",
        "\n",
        "asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS2qXcfnZwYg",
        "outputId": "fe7d5cb8-0714-4534-b706-295e10c0b5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-3.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-4.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-4.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-1.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-2.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-12.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-8.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-2.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-14.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-7.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-8.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-9.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-9.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-4.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-6.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-6.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-3.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-7.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-1.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-15.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-1.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-5.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-9.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-10.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-13.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-11.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-16.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-12.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-20.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-13.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-17.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-7.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-10.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-22.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-20.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-11.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-18.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-16.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-20.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-22.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-14.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-22.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-11.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-13.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-19.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-5.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-21.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-6.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-12.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-15.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-18.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-19.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-23.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-0.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-10.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-18.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-16.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-21.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-23.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-19.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-15.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-8.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-23.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-3.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-17.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-5.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-0.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-04-0.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-17.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-02-21.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-03-14.json.gz\n",
            "[extract] downloading data from https://data.gharchive.org/2021-01-01-2.json.gz\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] loading data to bucket github-activitiy-gb\n",
            "[extract] Uploaded 72 files successfully\n"
          ]
        }
      ],
      "source": [
        "!python extract_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incremental Loading of Data into S3 using AWS Lambda"
      ],
      "metadata": {
        "id": "yG4KpuhGYo8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You need to ensure that all the 3rd party libraries which are supposed to be deployed along with lambda functions are downloaded to a folder. In our case it is libs.\n",
        "\n",
        "* We need to go to the folder to build the zip file. Make sure the zip file is created in the base directory of the project and update the zip file with source code.\n",
        "\n",
        "* ```\n",
        "rm ghactivity-downloader.zip # remove current zip\n",
        "cd libs\n",
        "zip -r ../ghactivity-downloader.zip .\n",
        "cd ..\n",
        "zip -g ghactivity-downloader.zip lambda_function.py download.py\n",
        "```\n",
        "\n",
        "* We can upload the zip file to AWS Lambda console and validate successfully. Make sure to increase memory size appropriately."
      ],
      "metadata": {
        "id": "lsE13yFSZVBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p aws_lambda"
      ],
      "metadata": {
        "id": "Q0n6TY94YxHI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uxZiQ1G1Ja49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01531023-d7bd-4126-beac-5dae9d112ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aws_lambda/download.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile aws_lambda/download.py\n",
        "import requests\n",
        "\n",
        "def download_file(file):\n",
        "  res = requests.get(f'https://data.gharchive.org/{file}')\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aws_lambda/upload.py\n",
        "\n",
        "import boto3\n",
        " \n",
        "def get_client():\n",
        "  return boto3.client('s3')\n",
        " \n",
        " \n",
        "def upload_s3(body, bucket, file):\n",
        "  s3_client = get_client()\n",
        "  res = s3_client.put_object(\n",
        "    Bucket=bucket,\n",
        "    Key=file,\n",
        "    Body=body\n",
        "  )\n",
        "  return res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOWfGqGXsSAm",
        "outputId": "033a5afa-1a05-4d53-dd06-7c66f4ecacce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aws_lambda/upload.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aws_lambda/utils.py\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta as td\n",
        "import requests, boto3, os\n",
        "from botocore.errorfactory import ClientError\n",
        "\n",
        "\n",
        "def get_client():\n",
        "    \"\"\"\n",
        "    Returns AWS S3 client\n",
        "    \"\"\"\n",
        "    return boto3.client('s3')\n",
        "\n",
        "\n",
        "def get_prev_filename(bucket, file_prefix, bookmark_file, baseline_file):\n",
        "    \"\"\"\n",
        "    Get previous filename by reading bookmark file from S3\n",
        "    If bookmark file doesn't exist yet, use the baseline file\n",
        "    \"\"\"\n",
        "    s3_client = get_client()\n",
        "    try:\n",
        "        bookmark_file = s3_client.get_object(\n",
        "            Bucket=bucket,\n",
        "            Key=f'{file_prefix}/{bookmark_file}'\n",
        "        )\n",
        "\n",
        "        prev_file = bookmark_file['Body'].read().decode('utf-8')\n",
        "    \n",
        "    except ClientError as e:\n",
        "        if e.response['Error']['Code'] == 'NoSuchKey':\n",
        "            prev_file = baseline_file\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "    return prev_file\n",
        "\n",
        "\n",
        "def get_next_filename(prev_file):\n",
        "    \"\"\"\n",
        "    Get next filename by adding one hour to prev_file date\n",
        "    \"\"\"\n",
        "    dt_part = prev_file.split('.')[0]\n",
        "    next_dt = dt.strptime(dt_part, '%Y-%M-%d-%H') + td(hours=1)\n",
        "    next_filename = f\"{dt.strftime(next_dt, '%Y-%M-%d-%-H')}.json.gz\"\n",
        "    return next_filename\n",
        "\n",
        "\n",
        "def upload_bookmark(bucket, file_prefix, bookmark_file, bookmark_contents):\n",
        "    \"\"\"\n",
        "    Create bookmark file and place it on given S3 bucket\n",
        "    \"\"\"\n",
        "    s3_client = get_client()\n",
        "    s3_client.put_object(\n",
        "        Bucket=bucket,\n",
        "        Key=f'{file_prefix}/{bookmark_file}',\n",
        "        Body=bookmark_contents.encode('utf-8')\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEG2_A0VuePH",
        "outputId": "89852ec4-8eba-4817-8aae-035dbb2adbd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aws_lambda/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aws_lambda/lambda_function.py\n",
        "import os\n",
        "import boto3\n",
        "from download import download_file\n",
        "from upload import upload_s3\n",
        "from utils import get_prev_filename, get_next_filename\n",
        "\n",
        " \n",
        "def lambda_handler(event, context):\n",
        "    # Run the code in current environment to get appropriate permissions\n",
        "    environ = os.environ.get('ENVIRON')\n",
        "    if environ == 'DEV':\n",
        "        print(f'Running in {environ} environment')\n",
        "        os.environ.setdefault('AWS_PROFILE', 'glad')\n",
        "\n",
        "    # Get the environment variables\n",
        "    bucket = os.environ.get('BUCKET_NAME')\n",
        "    file_prefix = os.environ.get('FILE_PREFIX')\n",
        "    bookmark_file = os.environ.get('BOOKMARK_FILE')\n",
        "    baseline_file = os.environ.get('BASELINE_FILE')\n",
        "\n",
        "    # download and incrementally upload file to S3\n",
        "    while True:\n",
        "        prev_filename = get_prev_filename(bucket, file_prefix, bookmark_file, baseline_file)\n",
        "        filename = get_next_filename(prev_filename)\n",
        "        download_res = download_file(filename)\n",
        "        if download_res.status_code == 404:\n",
        "            print(f'Invalid file name or downloads caught up till {prev_filename}')\n",
        "            break\n",
        "\n",
        "        upload_res = upload_s3(\n",
        "            download_res.content,\n",
        "            bucket,\n",
        "            f'{file_prefix}/{filename}'\n",
        "        )\n",
        "        print(f'File {filename} successfully processed')\n",
        "\n",
        "        # update bookmark\n",
        "        upload_bookmark(bucket, file_prefix, bookmark_file, filename)\n",
        "\n",
        "    return upload_res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-bWNl-5sSDl",
        "outputId": "97fa7e74-f7ab-479a-e1c4-9ac1914e1765"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aws_lambda/lambda_function.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests -t aws_lambda/libs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLeVHvADQ5go",
        "outputId": "43482cbb-584d-45ca-fca7-43b5528efa63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 11.3 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 6.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed certifi-2021.10.8 charset-normalizer-2.0.12 idna-3.3 requests-2.27.1 urllib3-1.26.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aws_lambda/pack_code.sh\n",
        "#!/bin/sh\n",
        "\n",
        "# zip required code files along with dependencies \n",
        "CURRENT_DIR=$(pwd)\n",
        "cd libs\n",
        "zip -r ../github-activity-lambda.zip .\n",
        "cd ${CURRENT_DIR}\n",
        "zip -g github-activity-lambda.zip lambda_function.py download.py upload.py utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5sNzpxbqe4g",
        "outputId": "dd8035b1-7a0f-44ca-ede6-b7605c07bcbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aws_lambda/pack_code.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/aws_lambda\n",
        "!sh pack_lambda_code.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-upQCFTrhWC",
        "outputId": "07f48266-921f-4023-fb8a-1506d1242514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/aws_lambda\n",
            "  adding: requests/ (stored 0%)\n",
            "  adding: requests/exceptions.py (deflated 67%)\n",
            "  adding: requests/_internal_utils.py (deflated 51%)\n",
            "  adding: requests/structures.py (deflated 62%)\n",
            "  adding: requests/cookies.py (deflated 73%)\n",
            "  adding: requests/help.py (deflated 71%)\n",
            "  adding: requests/status_codes.py (deflated 60%)\n",
            "  adding: requests/__pycache__/ (stored 0%)\n",
            "  adding: requests/__pycache__/hooks.cpython-37.pyc (deflated 35%)\n",
            "  adding: requests/__pycache__/__init__.cpython-37.pyc (deflated 41%)\n",
            "  adding: requests/__pycache__/status_codes.cpython-37.pyc (deflated 47%)\n",
            "  adding: requests/__pycache__/cookies.cpython-37.pyc (deflated 60%)\n",
            "  adding: requests/__pycache__/certs.cpython-37.pyc (deflated 29%)\n",
            "  adding: requests/__pycache__/models.cpython-37.pyc (deflated 54%)\n",
            "  adding: requests/__pycache__/__version__.cpython-37.pyc (deflated 26%)\n",
            "  adding: requests/__pycache__/exceptions.cpython-37.pyc (deflated 65%)\n",
            "  adding: requests/__pycache__/auth.cpython-37.pyc (deflated 54%)\n",
            "  adding: requests/__pycache__/structures.cpython-37.pyc (deflated 55%)\n",
            "  adding: requests/__pycache__/_internal_utils.cpython-37.pyc (deflated 38%)\n",
            "  adding: requests/__pycache__/api.cpython-37.pyc (deflated 70%)\n",
            "  adding: requests/__pycache__/compat.cpython-37.pyc (deflated 40%)\n",
            "  adding: requests/__pycache__/help.cpython-37.pyc (deflated 43%)\n",
            "  adding: requests/__pycache__/packages.cpython-37.pyc (deflated 26%)\n",
            "  adding: requests/__pycache__/utils.cpython-37.pyc (deflated 51%)\n",
            "  adding: requests/__pycache__/adapters.cpython-37.pyc (deflated 58%)\n",
            "  adding: requests/__pycache__/sessions.cpython-37.pyc (deflated 57%)\n",
            "  adding: requests/api.py (deflated 74%)\n",
            "  adding: requests/packages.py (deflated 53%)\n",
            "  adding: requests/__version__.py (deflated 40%)\n",
            "  adding: requests/certs.py (deflated 35%)\n",
            "  adding: requests/hooks.py (deflated 50%)\n",
            "  adding: requests/__init__.py (deflated 63%)\n",
            "  adding: requests/adapters.py (deflated 76%)\n",
            "  adding: requests/utils.py (deflated 68%)\n",
            "  adding: requests/compat.py (deflated 66%)\n",
            "  adding: requests/sessions.py (deflated 72%)\n",
            "  adding: requests/auth.py (deflated 71%)\n",
            "  adding: requests/models.py (deflated 71%)\n",
            "  adding: idna-3.3.dist-info/ (stored 0%)\n",
            "  adding: idna-3.3.dist-info/top_level.txt (stored 0%)\n",
            "  adding: idna-3.3.dist-info/LICENSE.md (deflated 47%)\n",
            "  adding: idna-3.3.dist-info/METADATA (deflated 61%)\n",
            "  adding: idna-3.3.dist-info/WHEEL (stored 0%)\n",
            "  adding: idna-3.3.dist-info/RECORD (deflated 47%)\n",
            "  adding: idna-3.3.dist-info/INSTALLER (stored 0%)\n",
            "  adding: idna/ (stored 0%)\n",
            "  adding: idna/idnadata.py (deflated 79%)\n",
            "  adding: idna/core.py (deflated 77%)\n",
            "  adding: idna/intranges.py (deflated 57%)\n",
            "  adding: idna/__pycache__/ (stored 0%)\n",
            "  adding: idna/__pycache__/core.cpython-37.pyc (deflated 51%)\n",
            "  adding: idna/__pycache__/__init__.cpython-37.pyc (deflated 41%)\n",
            "  adding: idna/__pycache__/idnadata.cpython-37.pyc (deflated 67%)\n",
            "  adding: idna/__pycache__/uts46data.cpython-37.pyc (deflated 80%)\n",
            "  adding: idna/__pycache__/compat.cpython-37.pyc (deflated 37%)\n",
            "  adding: idna/__pycache__/intranges.cpython-37.pyc (deflated 40%)\n",
            "  adding: idna/__pycache__/codec.cpython-37.pyc (deflated 58%)\n",
            "  adding: idna/__pycache__/package_data.cpython-37.pyc (deflated 14%)\n",
            "  adding: idna/uts46data.py (deflated 80%)\n",
            "  adding: idna/codec.py (deflated 76%)\n",
            "  adding: idna/__init__.py (deflated 67%)\n",
            "  adding: idna/package_data.py (stored 0%)\n",
            "  adding: idna/compat.py (deflated 40%)\n",
            "  adding: idna/py.typed (stored 0%)\n",
            "  adding: bin/ (stored 0%)\n",
            "  adding: bin/normalizer (deflated 27%)\n",
            "  adding: urllib3-1.26.9.dist-info/ (stored 0%)\n",
            "  adding: urllib3-1.26.9.dist-info/top_level.txt (stored 0%)\n",
            "  adding: urllib3-1.26.9.dist-info/METADATA (deflated 65%)\n",
            "  adding: urllib3-1.26.9.dist-info/WHEEL (deflated 14%)\n",
            "  adding: urllib3-1.26.9.dist-info/LICENSE.txt (deflated 41%)\n",
            "  adding: urllib3-1.26.9.dist-info/RECORD (deflated 62%)\n",
            "  adding: urllib3-1.26.9.dist-info/INSTALLER (stored 0%)\n",
            "  adding: certifi/ (stored 0%)\n",
            "  adding: certifi/cacert.pem (deflated 46%)\n",
            "  adding: certifi/core.py (deflated 57%)\n",
            "  adding: certifi/__pycache__/ (stored 0%)\n",
            "  adding: certifi/__pycache__/core.cpython-37.pyc (deflated 38%)\n",
            "  adding: certifi/__pycache__/__init__.cpython-37.pyc (deflated 12%)\n",
            "  adding: certifi/__pycache__/__main__.cpython-37.pyc (deflated 23%)\n",
            "  adding: certifi/__init__.py (stored 0%)\n",
            "  adding: certifi/__main__.py (deflated 39%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/ (stored 0%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/LICENSE (deflated 41%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/entry_points.txt (deflated 21%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/top_level.txt (stored 0%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/METADATA (deflated 59%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/WHEEL (stored 0%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/RECORD (deflated 59%)\n",
            "  adding: charset_normalizer-2.0.12.dist-info/INSTALLER (stored 0%)\n",
            "  adding: urllib3/ (stored 0%)\n",
            "  adding: urllib3/exceptions.py (deflated 69%)\n",
            "  adding: urllib3/connection.py (deflated 71%)\n",
            "  adding: urllib3/_version.py (stored 0%)\n",
            "  adding: urllib3/filepost.py (deflated 63%)\n",
            "  adding: urllib3/_collections.py (deflated 69%)\n",
            "  adding: urllib3/__pycache__/ (stored 0%)\n",
            "  adding: urllib3/__pycache__/response.cpython-37.pyc (deflated 56%)\n",
            "  adding: urllib3/__pycache__/request.cpython-37.pyc (deflated 55%)\n",
            "  adding: urllib3/__pycache__/__init__.cpython-37.pyc (deflated 40%)\n",
            "  adding: urllib3/__pycache__/_version.cpython-37.pyc (deflated 18%)\n",
            "  adding: urllib3/__pycache__/filepost.cpython-37.pyc (deflated 46%)\n",
            "  adding: urllib3/__pycache__/fields.cpython-37.pyc (deflated 55%)\n",
            "  adding: urllib3/__pycache__/exceptions.cpython-37.pyc (deflated 65%)\n",
            "  adding: urllib3/__pycache__/_collections.cpython-37.pyc (deflated 54%)\n",
            "  adding: urllib3/__pycache__/poolmanager.cpython-37.pyc (deflated 55%)\n",
            "  adding: urllib3/__pycache__/connectionpool.cpython-37.pyc (deflated 56%)\n",
            "  adding: urllib3/__pycache__/connection.cpython-37.pyc (deflated 49%)\n",
            "  adding: urllib3/request.py (deflated 67%)\n",
            "  adding: urllib3/contrib/ (stored 0%)\n",
            "  adding: urllib3/contrib/_securetransport/ (stored 0%)\n",
            "  adding: urllib3/contrib/_securetransport/__pycache__/ (stored 0%)\n",
            "  adding: urllib3/contrib/_securetransport/__pycache__/__init__.cpython-37.pyc (deflated 19%)\n",
            "  adding: urllib3/contrib/_securetransport/__pycache__/low_level.cpython-37.pyc (deflated 49%)\n",
            "  adding: urllib3/contrib/_securetransport/__pycache__/bindings.cpython-37.pyc (deflated 53%)\n",
            "  adding: urllib3/contrib/_securetransport/bindings.py (deflated 75%)\n",
            "  adding: urllib3/contrib/_securetransport/low_level.py (deflated 68%)\n",
            "  adding: urllib3/contrib/_securetransport/__init__.py (stored 0%)\n",
            "  adding: urllib3/contrib/ntlmpool.py (deflated 64%)\n",
            "  adding: urllib3/contrib/appengine.py (deflated 71%)\n",
            "  adding: urllib3/contrib/_appengine_environ.py (deflated 54%)\n",
            "  adding: urllib3/contrib/__pycache__/ (stored 0%)\n",
            "  adding: urllib3/contrib/__pycache__/appengine.cpython-37.pyc (deflated 51%)\n",
            "  adding: urllib3/contrib/__pycache__/__init__.cpython-37.pyc (deflated 21%)\n",
            "  adding: urllib3/contrib/__pycache__/_appengine_environ.cpython-37.pyc (deflated 44%)\n",
            "  adding: urllib3/contrib/__pycache__/socks.cpython-37.pyc (deflated 50%)\n",
            "  adding: urllib3/contrib/__pycache__/securetransport.cpython-37.pyc (deflated 54%)\n",
            "  adding: urllib3/contrib/__pycache__/ntlmpool.cpython-37.pyc (deflated 41%)\n",
            "  adding: urllib3/contrib/__pycache__/pyopenssl.cpython-37.pyc (deflated 53%)\n",
            "  adding: urllib3/contrib/pyopenssl.py (deflated 69%)\n",
            "  adding: urllib3/contrib/__init__.py (stored 0%)\n",
            "  adding: urllib3/contrib/socks.py (deflated 70%)\n",
            "  adding: urllib3/contrib/securetransport.py (deflated 72%)\n",
            "  adding: urllib3/packages/ (stored 0%)\n",
            "  adding: urllib3/packages/__pycache__/ (stored 0%)\n",
            "  adding: urllib3/packages/__pycache__/__init__.cpython-37.pyc (deflated 21%)\n",
            "  adding: urllib3/packages/__pycache__/six.cpython-37.pyc (deflated 58%)\n",
            "  adding: urllib3/packages/six.py (deflated 75%)\n",
            "  adding: urllib3/packages/__init__.py (stored 0%)\n",
            "  adding: urllib3/packages/backports/ (stored 0%)\n",
            "  adding: urllib3/packages/backports/makefile.py (deflated 59%)\n",
            "  adding: urllib3/packages/backports/__pycache__/ (stored 0%)\n",
            "  adding: urllib3/packages/backports/__pycache__/__init__.cpython-37.pyc (deflated 20%)\n",
            "  adding: urllib3/packages/backports/__pycache__/makefile.cpython-37.pyc (deflated 33%)\n",
            "  adding: urllib3/packages/backports/__init__.py (stored 0%)\n",
            "  adding: urllib3/connectionpool.py (deflated 73%)\n",
            "  adding: urllib3/__init__.py (deflated 60%)\n",
            "  adding: urllib3/util/ (stored 0%)\n",
            "  adding: urllib3/util/connection.py (deflated 60%)\n",
            "  adding: urllib3/util/wait.py (deflated 63%)\n",
            "  adding: urllib3/util/queue.py (deflated 49%)\n",
            "  adding: urllib3/util/ssl_.py (deflated 66%)\n",
            "  adding: urllib3/util/ssltransport.py (deflated 70%)\n",
            "  adding: urllib3/util/__pycache__/ (stored 0%)\n",
            "  adding: urllib3/util/__pycache__/response.cpython-37.pyc (deflated 42%)\n",
            "  adding: urllib3/util/__pycache__/request.cpython-37.pyc (deflated 47%)\n",
            "  adding: urllib3/util/__pycache__/__init__.cpython-37.pyc (deflated 34%)\n",
            "  adding: urllib3/util/__pycache__/retry.cpython-37.pyc (deflated 59%)\n",
            "  adding: urllib3/util/__pycache__/wait.cpython-37.pyc (deflated 48%)\n",
            "  adding: urllib3/util/__pycache__/proxy.cpython-37.pyc (deflated 41%)\n",
            "  adding: urllib3/util/__pycache__/queue.cpython-37.pyc (deflated 45%)\n",
            "  adding: urllib3/util/__pycache__/ssltransport.cpython-37.pyc (deflated 54%)\n",
            "  adding: urllib3/util/__pycache__/timeout.cpython-37.pyc (deflated 60%)\n",
            "  adding: urllib3/util/__pycache__/ssl_match_hostname.cpython-37.pyc (deflated 38%)\n",
            "  adding: urllib3/util/__pycache__/ssl_.cpython-37.pyc (deflated 50%)\n",
            "  adding: urllib3/util/__pycache__/connection.cpython-37.pyc (deflated 40%)\n",
            "  adding: urllib3/util/__pycache__/url.cpython-37.pyc (deflated 46%)\n",
            "  adding: urllib3/util/ssl_match_hostname.py (deflated 60%)\n",
            "  adding: urllib3/util/request.py (deflated 64%)\n",
            "  adding: urllib3/util/retry.py (deflated 73%)\n",
            "  adding: urllib3/util/proxy.py (deflated 62%)\n",
            "  adding: urllib3/util/__init__.py (deflated 59%)\n",
            "  adding: urllib3/util/url.py (deflated 66%)\n",
            "  adding: urllib3/util/response.py (deflated 61%)\n",
            "  adding: urllib3/util/timeout.py (deflated 70%)\n",
            "  adding: urllib3/response.py (deflated 73%)\n",
            "  adding: urllib3/fields.py (deflated 70%)\n",
            "  adding: urllib3/poolmanager.py (deflated 71%)\n",
            "  adding: requests-2.27.1.dist-info/ (stored 0%)\n",
            "  adding: requests-2.27.1.dist-info/LICENSE (deflated 65%)\n",
            "  adding: requests-2.27.1.dist-info/REQUESTED (stored 0%)\n",
            "  adding: requests-2.27.1.dist-info/top_level.txt (stored 0%)\n",
            "  adding: requests-2.27.1.dist-info/METADATA (deflated 58%)\n",
            "  adding: requests-2.27.1.dist-info/WHEEL (deflated 14%)\n",
            "  adding: requests-2.27.1.dist-info/RECORD (deflated 55%)\n",
            "  adding: requests-2.27.1.dist-info/INSTALLER (stored 0%)\n",
            "  adding: charset_normalizer/ (stored 0%)\n",
            "  adding: charset_normalizer/cli/ (stored 0%)\n",
            "  adding: charset_normalizer/cli/normalizer.py (deflated 73%)\n",
            "  adding: charset_normalizer/cli/__pycache__/ (stored 0%)\n",
            "  adding: charset_normalizer/cli/__pycache__/__init__.cpython-37.pyc (deflated 18%)\n",
            "  adding: charset_normalizer/cli/__pycache__/normalizer.cpython-37.pyc (deflated 44%)\n",
            "  adding: charset_normalizer/cli/__init__.py (stored 0%)\n",
            "  adding: charset_normalizer/cd.py (deflated 74%)\n",
            "  adding: charset_normalizer/constant.py (deflated 70%)\n",
            "  adding: charset_normalizer/__pycache__/ (stored 0%)\n",
            "  adding: charset_normalizer/__pycache__/md.cpython-37.pyc (deflated 64%)\n",
            "  adding: charset_normalizer/__pycache__/__init__.cpython-37.pyc (deflated 41%)\n",
            "  adding: charset_normalizer/__pycache__/models.cpython-37.pyc (deflated 58%)\n",
            "  adding: charset_normalizer/__pycache__/cd.cpython-37.pyc (deflated 50%)\n",
            "  adding: charset_normalizer/__pycache__/constant.cpython-37.pyc (deflated 53%)\n",
            "  adding: charset_normalizer/__pycache__/legacy.cpython-37.pyc (deflated 55%)\n",
            "  adding: charset_normalizer/__pycache__/version.cpython-37.pyc (deflated 13%)\n",
            "  adding: charset_normalizer/__pycache__/api.cpython-37.pyc (deflated 48%)\n",
            "  adding: charset_normalizer/__pycache__/utils.cpython-37.pyc (deflated 51%)\n",
            "  adding: charset_normalizer/legacy.py (deflated 70%)\n",
            "  adding: charset_normalizer/api.py (deflated 75%)\n",
            "  adding: charset_normalizer/version.py (deflated 19%)\n",
            "  adding: charset_normalizer/md.py (deflated 81%)\n",
            "  adding: charset_normalizer/__init__.py (deflated 53%)\n",
            "  adding: charset_normalizer/utils.py (deflated 75%)\n",
            "  adding: charset_normalizer/assets/ (stored 0%)\n",
            "  adding: charset_normalizer/assets/__pycache__/ (stored 0%)\n",
            "  adding: charset_normalizer/assets/__pycache__/__init__.cpython-37.pyc (deflated 60%)\n",
            "  adding: charset_normalizer/assets/__init__.py (deflated 89%)\n",
            "  adding: charset_normalizer/models.py (deflated 73%)\n",
            "  adding: charset_normalizer/py.typed (stored 0%)\n",
            "  adding: certifi-2021.10.8.dist-info/ (stored 0%)\n",
            "  adding: certifi-2021.10.8.dist-info/LICENSE (deflated 43%)\n",
            "  adding: certifi-2021.10.8.dist-info/top_level.txt (stored 0%)\n",
            "  adding: certifi-2021.10.8.dist-info/METADATA (deflated 59%)\n",
            "  adding: certifi-2021.10.8.dist-info/WHEEL (deflated 14%)\n",
            "  adding: certifi-2021.10.8.dist-info/RECORD (deflated 44%)\n",
            "  adding: certifi-2021.10.8.dist-info/INSTALLER (stored 0%)\n",
            "  adding: lambda_function.py (deflated 59%)\n",
            "  adding: download.py (deflated 16%)\n",
            "  adding: upload.py (deflated 37%)\n",
            "  adding: utils.py (deflated 60%)\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automate using Events Bridge\n",
        "events bridge -> create rules -> GH-hourly -> every 60 mints -> lambda function -> function name -> create\n",
        "\n",
        "Monitor using Cloudwatch"
      ],
      "metadata": {
        "id": "RtepKxyZ8XgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2: Preprocessing using Spark"
      ],
      "metadata": {
        "id": "3yBl5lD98ykO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p spark"
      ],
      "metadata": {
        "id": "PH2ZjyuoQo7l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark/utils.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        " \n",
        "\n",
        "def get_spark_session(env, app_name):\n",
        "    \"\"\"\n",
        "    returns spark session\n",
        "    \"\"\"\n",
        "    # local cluster\n",
        "    if env == 'DEV':\n",
        "        spark = SparkSession. \\\n",
        "            builder. \\\n",
        "            master('local'). \\\n",
        "            appName(app_name). \\\n",
        "            getOrCreate()\n",
        "        return spark\n",
        "\n",
        "    # production cluster with yarn as resource manager\n",
        "    elif env == 'PROD':\n",
        "        spark = SparkSession. \\\n",
        "            builder. \\\n",
        "            master('yarn'). \\\n",
        "            appName(app_name). \\\n",
        "            getOrCreate()\n",
        "        return spark\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "452ZirzefTuC",
        "outputId": "4520d209-8631-4a87-b19e-49fbe9b8d3d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spark/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark/read.py\n",
        "\n",
        "def from_files(spark, data_dir, file_pattern, file_format):\n",
        "    \"\"\"\n",
        "    Reads files in given directory and returns spark dataframe\n",
        "    @spark spark session object\n",
        "    @data_dir directory to read files from\n",
        "    @file_pattern prefix for files\n",
        "    @file_format one of csv, json, or parquet\n",
        "    @returns spark dataframe\n",
        "    \"\"\"\n",
        "    df = spark. \\\n",
        "        read. \\\n",
        "        format(file_format). \\\n",
        "        load(f'{data_dir}/{file_pattern}')\n",
        "    return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr6XrVBsfv5Z",
        "outputId": "12bc9867-2ad4-48d8-fabf-50f9a1356daf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spark/read.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark/transform.py\n",
        "\n",
        "from pyspark.sql.functions import year, \\\n",
        "    month, dayofmonth\n",
        " \n",
        " \n",
        "def transform(df):\n",
        "    \"\"\"\n",
        "    augment dataframe with year, month, day columns\n",
        "    \"\"\"\n",
        "    return df.withColumn('year', year('created_at')). \\\n",
        "        withColumn('month', month('created_at')). \\\n",
        "        withColumn('day', dayofmonth('created_at'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTWg7t7afv1O",
        "outputId": "e9569d29-091c-4ff6-88e7-94d8f9581d3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spark/transform.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark/write.py\n",
        "\n",
        "def to_files(df, tgt_dir, file_format):\n",
        "    \"\"\"\n",
        "    writes dataframe to target directory\n",
        "    @df dataframe to write\n",
        "    @tgt_dir target directory\n",
        "    @file_format one of csv, json or parquet\n",
        "    \"\"\"\n",
        "    df.coalesce(16). \\\n",
        "        write. \\\n",
        "        partitionBy('year', 'month', 'day'). \\\n",
        "        mode('append'). \\\n",
        "        format(file_format). \\\n",
        "        save(tgt_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF-z8UlxoSBJ",
        "outputId": "decd2fe1-85fd-4fef-c8f3-dc5c94d4317a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spark/write.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark/app.py\n",
        "import os\n",
        "from utils import get_spark_session\n",
        "from read import from_files\n",
        "from transform import transform\n",
        "from write import to_files\n",
        " \n",
        " \n",
        "def main():\n",
        "    \"\"\"\n",
        "    main driver program\n",
        "    \"\"\"\n",
        "    # set environment variables\n",
        "    env = os.environ.get('ENVIRON')\n",
        "    src_dir = os.environ.get('SRC_DIR')\n",
        "    file_pattern = f\"{os.environ.get('SRC_FILE_PATTERN')}-*\"\n",
        "    src_file_format = os.environ.get('SRC_FILE_FORMAT')\n",
        "    tgt_dir = os.environ.get('TGT_DIR')\n",
        "    tgt_file_format = os.environ.get('TGT_FILE_FORMAT')\n",
        "\n",
        "    # create spark session\n",
        "    spark = get_spark_session(env, 'GitHub Activity - Reading Data')\n",
        "\n",
        "    # read files\n",
        "    df = from_files(spark, src_dir, file_pattern, src_file_format)\n",
        "\n",
        "    # preprocess and transform\n",
        "    df_transformed = transform(df)\n",
        "\n",
        "    # store transformed dataframe\n",
        "    to_files(df_transformed, tgt_dir, tgt_file_format)\n",
        "\n",
        "    df_transformed.printSchema()\n",
        "    df_transformed.select('repo.*').show()\n",
        " \n",
        " \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ41sSY_fvX8",
        "outputId": "c30b34d6-6c1f-4852-a2af-f532c67645d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spark/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy using Spark Client Mode"
      ],
      "metadata": {
        "id": "O37iIVEVYVFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark/spark-run.sh\n",
        "#!/bin/sh\n",
        "\n",
        "# set environment variables\n",
        "# make sure SRC_DIR and TGT_DIR are correct before running\n",
        "export ENVIRON=PROD\n",
        "export SRC_DIR=/user/${USER}/github-activity-gb/landing/ghactivity\n",
        "export SRC_FILE_FORMAT=json\n",
        "export TGT_DIR=/user/${USER}/github-activity-gb/raw/ghactivity\n",
        "export TGT_FILE_FORMAT=parquet\n",
        " \n",
        "export PYSPARK_PYTHON=python3\n",
        "\n",
        "# running for day 1\n",
        "export SRC_FILE_PATTERN=2022-04-01\n",
        " \n",
        "spark2-submit --master yarn \\\n",
        "    --py-files ghactivity.zip \\\n",
        "    app.py\n",
        " \n",
        "# running for day 2\n",
        "export SRC_FILE_PATTERN=2022-04-02\n",
        " \n",
        "spark2-submit --master yarn \\\n",
        "    --py-files ghactivity.zip \\\n",
        "    app.py\n",
        " \n",
        "# running for day 3\n",
        "export SRC_FILE_PATTERN=2022-04-03\n",
        " \n",
        "spark2-submit --master yarn \\\n",
        "    --py-files ghactivity.zip \\\n",
        "    app.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GEqdcedfvyG",
        "outputId": "47a86b06-99d1-46dc-ae0d-7b94a071c8d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spark/spark-run.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile spark/test.py\n",
        "from pyspark.sql.functions import to_date\n",
        "from utils import get_spark_session\n",
        "import getpass\n",
        "\n",
        "username = getpass.getuser()\n",
        "\n",
        "env = os.environ.get('ENVIRON')\n",
        "spark = get_spark_session(env, 'GitHub Activity - Reading Data')\n",
        "\n",
        " \n",
        "src_file_path = f'/user/{username}/github-activity/landing/ghactivity'\n",
        "src_df = spark.read.json(src_file_path)\n",
        "src_df.printSchema()\n",
        "src_df.show()\n",
        "src_df.count()\n",
        "\n",
        "src_df.groupBy(to_date('created_at').alias('created_at')).count().show()\n",
        " \n",
        "tgt_file_path = f'/user/{username}/github-activity/raw/ghactivity'\n",
        "tgt_df = spark.read.parquet(tgt_file_path)\n",
        "tgt_df.printSchema()\n",
        "tgt_df.show()\n",
        "tgt_df.count()\n",
        "tgt_df.groupBy('year', 'month', 'day').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1e52UZRfvu7",
        "outputId": "695ba159-a5f1-47a4-9ffa-4e334c880b66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing spark/test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy using Spark Cluster Mode\n",
        "\n",
        "```\n",
        "spark-submit \\\n",
        "    --master yarn \\\n",
        "    --deploy-mode cluster \\\n",
        "    --conf \"spark.yarn.appMasterEnv.ENVIRON=PROD\" \\\n",
        "    --conf \"spark.yarn.appMasterEnv.SRC_DIR=/user/hadoop/prod/landing/ghactivity\" \\\n",
        "    --conf \"spark.yarn.appMasterEnv.SRC_FILE_FORMAT=json\" \\\n",
        "    --conf \"spark.yarn.appMasterEnv.TGT_DIR=/user/hadoop/prod/raw/ghactivity/\" \\\n",
        "    --conf \"spark.yarn.appMasterEnv.TGT_FILE_FORMAT=parquet\" \\\n",
        "    --conf \"spark.yarn.appMasterEnv.SRC_FILE_PREFIX=2021-01-15 \\\n",
        "    --py-files ghactivity.zip\\\n",
        "    app.py\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "m9bchrJlmJxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy using AWS EMR Step Executions"
      ],
      "metadata": {
        "id": "Ra6RXYPinVyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3: Automate ETL using AWS Glue\n",
        "\n",
        "1.   First, we create Glue crawler to the S3 landing folder. \n",
        "\n",
        "2.   Attach a custom policy to the  AWSGlueServiceRole-GitHub role we created during crawler, with read, write, delete permissions to the bucket where data is stored.\n",
        "<br> We will also generate Spark Logs generated by GitHub related Glue Jobs in the same bucket. \n",
        "```\n",
        "{\n",
        "    \"Version\": \"2012-10-17\",\n",
        "        \"Statement\": [\n",
        "        {\n",
        "            \"Effect\": \"Allow\",\n",
        "            \"Action\": [\n",
        "                \"s3:*Object\"\n",
        "            ],\n",
        "            \"Resource\": [\n",
        "                \"arn:aws:s3:::github-activity-gb/*\"\n",
        "            ]\n",
        "        }\n",
        "        ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kVE8I6tb5B_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Glue Spark Job to Partition data by Date column and Store in Parquet format"
      ],
      "metadata": {
        "id": "N_QFIEKBK7fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark History Server UI\n",
        "\n",
        "* When we submit Glue Jobs, it uses Spark under the hood. We need to access Spark UI to troubleshoot some of the issues.\n",
        "\n",
        "* There are several ways to access Spark UI. One of the approaches is to use Docker Container which contains Spark UI Server.\n",
        "\n",
        "* You can set up a local Docker based Spark UI Server using these instructions from AWS Glue Samples GitHub repository. Make sure to clone the repository before running any docker commands as the image is not available under docker hub.\n",
        "\n",
        " \n",
        "\n",
        "```\n",
        "git clone https://github.com/aws-samples/aws-glue-samples\n",
        "cd aws-glue-samples/utilities/Spark_UI\n",
        "docker build -t glue/sparkui:latest .\n",
        "```\n",
        "\n",
        "Once the container is built make sure to add required environment variables as [mentioned in the instructions before](https://github.com/aws-samples/aws-glue-samples/tree/master/utilities/Spark_UI#start-the-spark-history-server) starting the docker container with Spark UI Server.\n",
        "\n",
        "* Set LOG_DIR by replacing s3a://path_to_eventlog with your event log directory\n",
        "\n",
        "* Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY with your valid AWS credentials."
      ],
      "metadata": {
        "id": "-IL2Hv3G0qtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p glue"
      ],
      "metadata": {
        "id": "_KyiXDU4LI_r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile glue/preprocess_job.py\n",
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.functions import date_format, substring\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from awsglue.dynamicframe import DynamicFrame\n",
        " \n",
        "## @params: [JOB_NAME]\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        " \n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        " \n",
        "datasource0 = glueContext. \\\n",
        "  create_dynamic_frame. \\\n",
        "  from_catalog(\n",
        "    database = \"itvghlandingdb\",\n",
        "    table_name = \"ghactivitycsv\",\n",
        "    transformation_ctx = \"datasource0\"\n",
        "  )\n",
        " \n",
        "df = datasource0. \\\n",
        "  toDF(). \\\n",
        "  withColumn('year', date_format(substring('created_at', 1, 10), 'yyyy')). \\\n",
        "  withColumn('month', date_format(substring('created_at', 1, 10), 'MM')). \\\n",
        "  withColumn('day', date_format(substring('created_at', 1, 10), 'dd'))\n",
        " \n",
        "dyf = DynamicFrame.fromDF(dataframe=df, glue_ctx=glueContext, name=\"dyf\")\n",
        " \n",
        "datasink4 = glueContext. \\\n",
        "  write_dynamic_frame. \\\n",
        "  from_options(frame=dyf,\n",
        "    connection_type=\"s3\",\n",
        "    connection_options={\"path\": \"s3://github-activity-gb/raw/ghactivity/\",\n",
        "      \"compression\": \"snappy\",\n",
        "      \"partitionKeys\": [\"year\", \"month\", \"day\"]},\n",
        "    format=\"glueparquet\",\n",
        "    transformation_ctx=\"datasink4\")\n",
        " \n",
        "job.commit()"
      ],
      "metadata": {
        "id": "Hd-Mex4o0RqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad7b1b7-cede-40ee-a3bf-b5656c821de6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing glue/preprocess_job.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate using Athena\n",
        "\n",
        " Once the table structure is refreshed, we can validate using Athena by running some standard validation queries.\n",
        "\n",
        "1. Get the number of records from the table.\n",
        "\n",
        " `SELECT count(1) FROM githubdb.ghactivity;`\n",
        "\n",
        "2. Get the number of new repositories added.\n",
        "\n",
        " ```\n",
        "SELECT count(1), count(distinct repo.id) FROM githubdb.ghactivity\n",
        "WHERE type = 'CreateEvent'\n",
        "AND payload.ref_type = 'repository';\n",
        "```\n",
        "\n",
        "3. Preview repo related details using repo column of type struct.\n",
        " ```\n",
        "SELECT repo FROM githubdb.ghactivity\n",
        "WHERE type = 'CreateEvent'\n",
        "AND payload.ref_type = 'repository'\n",
        "LIMIT 10;\n",
        "```\n",
        "\n",
        "4. Get the number of repositories created for each of the 3 days.\n",
        "```\n",
        "SELECT substr(created_at, 1, 10), count(1), count(distinct id)\n",
        "FROM githubdb.ghactivity\n",
        "WHERE type = 'CreateEvent'\n",
        "AND payload.ref_type = 'repository'\n",
        "GROUP BY substr(created_at, 1, 10);\n",
        "```\n",
        "\n",
        " Here are some of the observations.\n",
        " *   As parquet is columnar storage, the performance will be relatively better compared to JSON.\n",
        " *   The compression rate with respect to snappy might be lower than compression rate with gzip against JSON.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ylhdeb3Tf_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Incremental ETL using Glue Bookmark"
      ],
      "metadata": {
        "id": "WKPYiEV2Z5oQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start by cleaning up previous data, and start over.\n",
        "\n",
        "```\n",
        "aws s3 ls s3://github-activity-gb/raw/ --profile glad\n",
        "aws s3 ls s3://github-activity-gb/raw/ghactivity/ --profile glad\n",
        "aws s3 rm s3://github-activity-gb/raw/ --recursive --profile glad\n",
        "```\n",
        "\n",
        "We can enable Glue Bookmart either at Job level or at Run level(which will be active only for that specific run).\n",
        "After running again, we can validate data using Athena queries again.\n",
        "\n",
        "* Listing jobs\n",
        "```\n",
        "aws glue list-jobs \\\n",
        "    --profile glad \\\n",
        "    --region ap-south-1\n",
        "    ```\n",
        "\n",
        "Get job details\n",
        "```\n",
        "aws glue \\\n",
        "    get-job \\\n",
        "        --job-name github_json_to_parquet \\\n",
        "        --profile glad \\\n",
        "        --region ap-south-1\n",
        "```\n",
        "Get job run ids. The latest one will be typically at top.\n",
        "```\n",
        "aws glue \\\n",
        "    get-job-runs \\\n",
        "        --job-name github_json_to_parquet \\\n",
        "        --profile glad \\\n",
        "        --region ap-south-1\n",
        "```\n",
        "Get job run details to verify if job is successful or not.\n",
        "```\n",
        "aws glue \\\n",
        "    get-job-run \\\n",
        "        --job-name github_json_to_parquet \\\n",
        "        --run-id jr_a350197ce2d5cc3168160813e28bef293e0edd4fc2fe8f458191885d0bb32f96 \\\n",
        "        --profile glad \\\n",
        "        --region ap-south-1\n",
        "```\n",
        "Get job bookmark details. This information will be used to read the data in incremental fashion in subsequent runs. Make sure to keep track of it to compare with subsequent runs.\n",
        "```\n",
        "aws glue \\\n",
        "    get-job-bookmark \\\n",
        "        --job-name github_json_to_parquet \\\n",
        "        --profile glad \\\n",
        "        --region ap-south-1\n",
        "```\n",
        "We can use reset-job-bookmark to reset remove the bookmark. It comes handy to start the jobs from the beginning. We can also reset to a particular run using run id.\n",
        "\n",
        "```\n",
        "aws glue reset-job-bookmark \\\n",
        "    --job-name github_json_to_parquet \\\n",
        "    --profile glad \\\n",
        "    --region ap-south-1\n",
        "```\n"
      ],
      "metadata": {
        "id": "TBm1ieoqqoVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate Data using Athena Queries\n",
        "We run the same athena queries as earlier and compare the results as well as performance.\n",
        "\n",
        "When we recrawl the table, we might run into a known issue with Athena. Refer to [this document](https://docs.amazonaws.cn/en_us/athena/latest/ug/updates-and-partitions.html&sa=D&source=editors&ust=1629529826660000&usg=AOvVaw2KKPih8iFjZ9ykQ8RgKtGs) about the details.\n",
        "\n",
        "We run the below scripts to drop the partitions and add them using Athena.\n",
        "\n",
        "```\n",
        "ALTER TABLE githubrawdb.ghactivity\n",
        "DROP PARTITION (year = '2021', month = '01', day = '16');\n",
        " \n",
        "MSCK REPAIR TABLE githubrawdb.ghactivity;\n",
        "```\n",
        "\n",
        "Get the number of records from the table.\n",
        "```\n",
        "SELECT count(1) FROM ghactivity;\n",
        "```\n",
        "\n",
        "Get the number of new repositories added.\n",
        "```\n",
        "SELECT count(1), count(distinct repo.id) FROM ghactivity\n",
        "WHERE type = 'CreateEvent'\n",
        "AND payload.ref_type = 'repository';\n",
        "```\n",
        "\n",
        "Preview repo related details using repo column of type struct.\n",
        "```\n",
        "SELECT repo FROM ghactivity\n",
        "WHERE type = 'CreateEvent'\n",
        "AND payload.ref_type = 'repository'\n",
        "LIMIT 10;\n",
        "```\n",
        "\n",
        "Get the number of repositories created for each of the 4 days. Make sure to compare with previous runs to ensure that counts do not change too much.\n",
        "```\n",
        "SELECT substr(created_at, 1, 10), count(1), count(distinct id) FROM ghactivity\n",
        "WHERE type = 'CreateEvent'\n",
        "AND payload.ref_type = 'repository'\n",
        "GROUP BY substr(created_at, 1, 10);\n",
        "```"
      ],
      "metadata": {
        "id": "Z1NlPq0TYp9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install pipreqs > /dev/null\n",
        "!pipreqs ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkZ09ZL78nmL",
        "outputId": "e2f28b2c-360e-48b7-ce7f-a02133cc4cc7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Successfully saved requirements file in ./requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r code2.zip aws_lambda glue spark extract_data.py"
      ],
      "metadata": {
        "id": "zXIftGMC0RiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -l code2.zip"
      ],
      "metadata": {
        "id": "rphlvI5B0Rek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e4676c-0d71-4e79-ccd0-7f876f4fb618"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  code2.zip\n",
            "  Length      Date    Time    Name\n",
            "---------  ---------- -----   ----\n",
            "        0  2022-05-10 07:41   aws_lambda/\n",
            "      221  2022-05-10 07:40   aws_lambda/upload.py\n",
            "     1374  2022-05-10 07:40   aws_lambda/lambda_function.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/idna-3.3.dist-info/\n",
            "        4  2022-05-10 07:40   aws_lambda/libs/idna-3.3.dist-info/INSTALLER\n",
            "     1523  2022-05-10 07:40   aws_lambda/libs/idna-3.3.dist-info/LICENSE.md\n",
            "     9765  2022-05-10 07:40   aws_lambda/libs/idna-3.3.dist-info/METADATA\n",
            "     1457  2022-05-10 07:40   aws_lambda/libs/idna-3.3.dist-info/RECORD\n",
            "        5  2022-05-10 07:40   aws_lambda/libs/idna-3.3.dist-info/top_level.txt\n",
            "       92  2022-05-10 07:40   aws_lambda/libs/idna-3.3.dist-info/WHEEL\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/certifi/\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/certifi/__pycache__/\n",
            "     1095  2022-05-10 07:40   aws_lambda/libs/certifi/__pycache__/core.cpython-37.pyc\n",
            "      391  2022-05-10 07:40   aws_lambda/libs/certifi/__pycache__/__main__.cpython-37.pyc\n",
            "      226  2022-05-10 07:40   aws_lambda/libs/certifi/__pycache__/__init__.cpython-37.pyc\n",
            "   265969  2022-05-10 07:40   aws_lambda/libs/certifi/cacert.pem\n",
            "     2303  2022-05-10 07:40   aws_lambda/libs/certifi/core.py\n",
            "       62  2022-05-10 07:40   aws_lambda/libs/certifi/__init__.py\n",
            "      243  2022-05-10 07:40   aws_lambda/libs/certifi/__main__.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/bin/\n",
            "      244  2022-05-10 07:40   aws_lambda/libs/bin/normalizer\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/requests/\n",
            "    10207  2022-05-10 07:40   aws_lambda/libs/requests/auth.py\n",
            "     3434  2022-05-10 07:40   aws_lambda/libs/requests/exceptions.py\n",
            "     3005  2022-05-10 07:40   aws_lambda/libs/requests/structures.py\n",
            "    21645  2022-05-10 07:40   aws_lambda/libs/requests/adapters.py\n",
            "     2054  2022-05-10 07:40   aws_lambda/libs/requests/compat.py\n",
            "     4188  2022-05-10 07:40   aws_lambda/libs/requests/status_codes.py\n",
            "     6402  2022-05-10 07:40   aws_lambda/libs/requests/api.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/\n",
            "     4176  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/status_codes.cpython-37.pyc\n",
            "      572  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/certs.cpython-37.pyc\n",
            "     4360  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/structures.cpython-37.pyc\n",
            "     2800  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/help.cpython-37.pyc\n",
            "    24004  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/utils.cpython-37.pyc\n",
            "     5880  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/exceptions.cpython-37.pyc\n",
            "    18738  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/cookies.cpython-37.pyc\n",
            "      662  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/packages.cpython-37.pyc\n",
            "    16903  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/adapters.cpython-37.pyc\n",
            "     1776  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/compat.cpython-37.pyc\n",
            "    24504  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/models.cpython-37.pyc\n",
            "    19619  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/sessions.cpython-37.pyc\n",
            "      506  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/__version__.cpython-37.pyc\n",
            "     3857  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/__init__.cpython-37.pyc\n",
            "      931  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/hooks.cpython-37.pyc\n",
            "     6638  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/api.cpython-37.pyc\n",
            "     1259  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/_internal_utils.cpython-37.pyc\n",
            "     8303  2022-05-10 07:40   aws_lambda/libs/requests/__pycache__/auth.cpython-37.pyc\n",
            "     1096  2022-05-10 07:40   aws_lambda/libs/requests/_internal_utils.py\n",
            "    29835  2022-05-10 07:40   aws_lambda/libs/requests/sessions.py\n",
            "    18430  2022-05-10 07:40   aws_lambda/libs/requests/cookies.py\n",
            "    33277  2022-05-10 07:40   aws_lambda/libs/requests/utils.py\n",
            "      441  2022-05-10 07:40   aws_lambda/libs/requests/__version__.py\n",
            "     4924  2022-05-10 07:40   aws_lambda/libs/requests/__init__.py\n",
            "      453  2022-05-10 07:40   aws_lambda/libs/requests/certs.py\n",
            "     3968  2022-05-10 07:40   aws_lambda/libs/requests/help.py\n",
            "    35051  2022-05-10 07:40   aws_lambda/libs/requests/models.py\n",
            "      932  2022-05-10 07:40   aws_lambda/libs/requests/packages.py\n",
            "      757  2022-05-10 07:40   aws_lambda/libs/requests/hooks.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/\n",
            "     8217  2022-05-10 07:40   aws_lambda/libs/urllib3/exceptions.py\n",
            "    20070  2022-05-10 07:40   aws_lambda/libs/urllib3/connection.py\n",
            "    10811  2022-05-10 07:40   aws_lambda/libs/urllib3/_collections.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/\n",
            "    25270  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/connectionpool.cpython-37.pyc\n",
            "    14996  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/poolmanager.cpython-37.pyc\n",
            "    13737  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/connection.cpython-37.pyc\n",
            "    12003  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/exceptions.cpython-37.pyc\n",
            "     2711  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/filepost.cpython-37.pyc\n",
            "     5511  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/request.cpython-37.pyc\n",
            "      164  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/_version.cpython-37.pyc\n",
            "     2132  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/__init__.cpython-37.pyc\n",
            "    10661  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/_collections.cpython-37.pyc\n",
            "     8093  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/fields.cpython-37.pyc\n",
            "    20553  2022-05-10 07:40   aws_lambda/libs/urllib3/__pycache__/response.cpython-37.pyc\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/util/\n",
            "    22001  2022-05-10 07:40   aws_lambda/libs/urllib3/util/retry.py\n",
            "     4901  2022-05-10 07:40   aws_lambda/libs/urllib3/util/connection.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/\n",
            "    16821  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/retry.cpython-37.pyc\n",
            "     1268  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/proxy.cpython-37.pyc\n",
            "     8850  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/timeout.cpython-37.pyc\n",
            "     3366  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/connection.cpython-37.pyc\n",
            "    11231  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/ssl_.cpython-37.pyc\n",
            "     3456  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/request.cpython-37.pyc\n",
            "     3081  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/wait.cpython-37.pyc\n",
            "     1060  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/__init__.cpython-37.pyc\n",
            "      991  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/queue.cpython-37.pyc\n",
            "     7256  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/ssltransport.cpython-37.pyc\n",
            "     2278  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/response.cpython-37.pyc\n",
            "    10586  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/url.cpython-37.pyc\n",
            "     3198  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__pycache__/ssl_match_hostname.cpython-37.pyc\n",
            "     5758  2022-05-10 07:40   aws_lambda/libs/urllib3/util/ssl_match_hostname.py\n",
            "    17165  2022-05-10 07:40   aws_lambda/libs/urllib3/util/ssl_.py\n",
            "     5404  2022-05-10 07:40   aws_lambda/libs/urllib3/util/wait.py\n",
            "    14030  2022-05-10 07:40   aws_lambda/libs/urllib3/util/url.py\n",
            "    10003  2022-05-10 07:40   aws_lambda/libs/urllib3/util/timeout.py\n",
            "     4225  2022-05-10 07:40   aws_lambda/libs/urllib3/util/request.py\n",
            "     3510  2022-05-10 07:40   aws_lambda/libs/urllib3/util/response.py\n",
            "     1155  2022-05-10 07:40   aws_lambda/libs/urllib3/util/__init__.py\n",
            "     6895  2022-05-10 07:40   aws_lambda/libs/urllib3/util/ssltransport.py\n",
            "     1605  2022-05-10 07:40   aws_lambda/libs/urllib3/util/proxy.py\n",
            "      498  2022-05-10 07:40   aws_lambda/libs/urllib3/util/queue.py\n",
            "       63  2022-05-10 07:40   aws_lambda/libs/urllib3/_version.py\n",
            "     2440  2022-05-10 07:40   aws_lambda/libs/urllib3/filepost.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/__pycache__/\n",
            "    27466  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/__pycache__/six.cpython-37.pyc\n",
            "      151  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/__pycache__/__init__.cpython-37.pyc\n",
            "    34666  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/six.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/__init__.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/backports/\n",
            "     1417  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/backports/makefile.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/backports/__pycache__/\n",
            "     1251  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/backports/__pycache__/makefile.cpython-37.pyc\n",
            "      161  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/backports/__pycache__/__init__.cpython-37.pyc\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/packages/backports/__init__.py\n",
            "     5985  2022-05-10 07:40   aws_lambda/libs/urllib3/request.py\n",
            "    28276  2022-05-10 07:40   aws_lambda/libs/urllib3/response.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/\n",
            "    34417  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/securetransport.py\n",
            "     4538  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/ntlmpool.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/\n",
            "     5520  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/socks.cpython-37.pyc\n",
            "     1359  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/_appengine_environ.cpython-37.pyc\n",
            "      150  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/__init__.cpython-37.pyc\n",
            "     8107  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/appengine.cpython-37.pyc\n",
            "    21409  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/securetransport.cpython-37.pyc\n",
            "    15353  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/pyopenssl.cpython-37.pyc\n",
            "     3556  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__pycache__/ntlmpool.cpython-37.pyc\n",
            "    16874  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/pyopenssl.py\n",
            "      957  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_appengine_environ.py\n",
            "     7097  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/socks.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/__pycache__/\n",
            "     8972  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/__pycache__/low_level.cpython-37.pyc\n",
            "    10601  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/__pycache__/bindings.cpython-37.pyc\n",
            "      167  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/__pycache__/__init__.cpython-37.pyc\n",
            "    13922  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/low_level.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/__init__.py\n",
            "    17632  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/_securetransport/bindings.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/__init__.py\n",
            "    11010  2022-05-10 07:40   aws_lambda/libs/urllib3/contrib/appengine.py\n",
            "     8579  2022-05-10 07:40   aws_lambda/libs/urllib3/fields.py\n",
            "     2763  2022-05-10 07:40   aws_lambda/libs/urllib3/__init__.py\n",
            "    19786  2022-05-10 07:40   aws_lambda/libs/urllib3/poolmanager.py\n",
            "    39013  2022-05-10 07:40   aws_lambda/libs/urllib3/connectionpool.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/\n",
            "        4  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/INSTALLER\n",
            "       77  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/entry_points.txt\n",
            "    11713  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/METADATA\n",
            "     2556  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/RECORD\n",
            "       19  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/top_level.txt\n",
            "     1070  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/LICENSE\n",
            "       92  2022-05-10 07:40   aws_lambda/libs/charset_normalizer-2.0.12.dist-info/WHEEL\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/idna/\n",
            "      321  2022-05-10 07:40   aws_lambda/libs/idna/compat.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/\n",
            "     9758  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/core.cpython-37.pyc\n",
            "   185499  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/uts46data.cpython-37.pyc\n",
            "      705  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/compat.cpython-37.pyc\n",
            "      866  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/__init__.cpython-37.pyc\n",
            "    23086  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/idnadata.cpython-37.pyc\n",
            "     3225  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/codec.cpython-37.pyc\n",
            "      162  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/package_data.cpython-37.pyc\n",
            "     1936  2022-05-10 07:40   aws_lambda/libs/idna/__pycache__/intranges.cpython-37.pyc\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/idna/py.typed\n",
            "   204400  2022-05-10 07:40   aws_lambda/libs/idna/uts46data.py\n",
            "    44025  2022-05-10 07:40   aws_lambda/libs/idna/idnadata.py\n",
            "    12795  2022-05-10 07:40   aws_lambda/libs/idna/core.py\n",
            "     1881  2022-05-10 07:40   aws_lambda/libs/idna/intranges.py\n",
            "      849  2022-05-10 07:40   aws_lambda/libs/idna/__init__.py\n",
            "       21  2022-05-10 07:40   aws_lambda/libs/idna/package_data.py\n",
            "     3374  2022-05-10 07:40   aws_lambda/libs/idna/codec.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/\n",
            "        4  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/INSTALLER\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/REQUESTED\n",
            "     4984  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/METADATA\n",
            "     2834  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/RECORD\n",
            "        9  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/top_level.txt\n",
            "    10142  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/LICENSE\n",
            "      110  2022-05-10 07:40   aws_lambda/libs/requests-2.27.1.dist-info/WHEEL\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/urllib3-1.26.9.dist-info/\n",
            "        4  2022-05-10 07:40   aws_lambda/libs/urllib3-1.26.9.dist-info/INSTALLER\n",
            "    46325  2022-05-10 07:40   aws_lambda/libs/urllib3-1.26.9.dist-info/METADATA\n",
            "     5741  2022-05-10 07:40   aws_lambda/libs/urllib3-1.26.9.dist-info/RECORD\n",
            "     1115  2022-05-10 07:40   aws_lambda/libs/urllib3-1.26.9.dist-info/LICENSE.txt\n",
            "        8  2022-05-10 07:40   aws_lambda/libs/urllib3-1.26.9.dist-info/top_level.txt\n",
            "      110  2022-05-10 07:40   aws_lambda/libs/urllib3-1.26.9.dist-info/WHEEL\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/\n",
            "    20303  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/api.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/\n",
            "      238  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/version.cpython-37.pyc\n",
            "     3293  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/legacy.cpython-37.pyc\n",
            "     7984  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/utils.cpython-37.pyc\n",
            "     8761  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/cd.cpython-37.pyc\n",
            "    13173  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/models.cpython-37.pyc\n",
            "     1689  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/__init__.cpython-37.pyc\n",
            "    15007  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/md.cpython-37.pyc\n",
            "    10912  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/api.cpython-37.pyc\n",
            "    13783  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__pycache__/constant.cpython-37.pyc\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/py.typed\n",
            "    18191  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/md.py\n",
            "    19449  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/constant.py\n",
            "    11076  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/cd.py\n",
            "     9308  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/utils.py\n",
            "     1790  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/__init__.py\n",
            "     3384  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/legacy.py\n",
            "       80  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/version.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/cli/\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/cli/__pycache__/\n",
            "      157  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/cli/__pycache__/__init__.cpython-37.pyc\n",
            "     6013  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/cli/__pycache__/normalizer.cpython-37.pyc\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/cli/__init__.py\n",
            "     9364  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/cli/normalizer.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/assets/\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/assets/__pycache__/\n",
            "     7953  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/assets/__pycache__/__init__.cpython-37.pyc\n",
            "    25485  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/assets/__init__.py\n",
            "    13303  2022-05-10 07:40   aws_lambda/libs/charset_normalizer/models.py\n",
            "        0  2022-05-10 07:40   aws_lambda/libs/certifi-2021.10.8.dist-info/\n",
            "        4  2022-05-10 07:40   aws_lambda/libs/certifi-2021.10.8.dist-info/INSTALLER\n",
            "     2994  2022-05-10 07:40   aws_lambda/libs/certifi-2021.10.8.dist-info/METADATA\n",
            "      942  2022-05-10 07:40   aws_lambda/libs/certifi-2021.10.8.dist-info/RECORD\n",
            "        8  2022-05-10 07:40   aws_lambda/libs/certifi-2021.10.8.dist-info/top_level.txt\n",
            "     1049  2022-05-10 07:40   aws_lambda/libs/certifi-2021.10.8.dist-info/LICENSE\n",
            "      110  2022-05-10 07:40   aws_lambda/libs/certifi-2021.10.8.dist-info/WHEEL\n",
            "     1505  2022-05-10 07:40   aws_lambda/utils.py\n",
            "      113  2022-05-10 07:40   aws_lambda/download.py\n",
            "      229  2022-05-10 07:41   aws_lambda/pack_code.sh\n",
            "        0  2022-05-10 07:42   glue/\n",
            "     1373  2022-05-10 07:42   glue/preprocess_job.py\n",
            "        0  2022-05-10 07:42   spark/\n",
            "      967  2022-05-10 07:41   spark/app.py\n",
            "      584  2022-05-10 07:41   spark/utils.py\n",
            "      446  2022-05-10 07:41   spark/read.py\n",
            "      365  2022-05-10 07:41   spark/write.py\n",
            "      674  2022-05-10 07:41   spark/test.py\n",
            "      315  2022-05-10 07:41   spark/transform.py\n",
            "      510  2022-05-10 07:42   spark/deploy.sh\n",
            "      734  2022-05-10 07:41   spark/spark-run.sh\n",
            "     2016  2022-05-10 07:39   extract_data.py\n",
            "---------                     -------\n",
            "  2105194                     243 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5DMIoLJV0RLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AJPb5O-rTfEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yXb57mBCTe_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TjNpV50BTe6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ETL Pipeline using AWS Lambda, Glue Jobs, EMR, Athena.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}